{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Challenger - Fase 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O comando abaixo baixa as bibliotecas nas versões que foram usadas nesse projeto, caso queira executar o notebook é importante criar um virtualenv no seu ambiente.\n",
    "na documentação oficial do python a seguir tem o passo a passo \n",
    "https://docs.python.org/pt-br/3/library/venv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1570,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arch\n",
    "import holidays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "import scipy.stats as stats\n",
    "from termcolor import colored\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de Utilidades das Bibliotecas Importadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Biblioteca | Descrição | \n",
    "|---|---|\n",
    "| arch | Modelagem de processos ARCH (Autoregressive Conditional Heteroskedasticity) para séries temporais financeiras. |\n",
    "| holidays | Identifica feriados em diferentes países. |\n",
    "| numpy (np) | Biblioteca fundamental para operações matemáticas e manipulação de arrays multidimensionais. |\n",
    "| pandas (pd) |  Biblioteca para análise e manipulação de dados em DataFrames e Series. |\n",
    "| seaborn (sns) |  Criação de visualizações estatísticas de alta qualidade com base em Matplotlib. |\n",
    "| xgboost (xgb) | Implementação altamente performática do algoritmo XGBoost para tarefas de aprendizagem supervisionada (regressão e classificação). |\n",
    "| pmdarima (pm) | Modelagem de séries temporais com modelos ARIMA (Autoregressive Integrated Moving Average) e SARIMA (Seasonal ARIMA) automatizados. |\n",
    "| date | Manipulação de datas. |\n",
    "| prophet | Biblioteca para previsão de séries temporais baseada no algoritmo Prophet do Facebook. | \n",
    "| scipy.stats (stats) | Funções estatísticas avançadas como testes de hipótese e distribuições de probabilidade. |\n",
    "| termcolor | Impressão de texto colorido no terminal. |\n",
    "| matplotlib (plt) | Biblioteca poderosa para criação de gráficos e visualização de dados. |\n",
    "| rcParams | Funções para configuração global de parâmetros do Matplotlib. |\n",
    "| prettytable | Criação de tabelas legíveis e formatadas para impressão no terminal. |\n",
    "| pmdarima.arima.auto_arima | Função para identificação automática de modelos ARIMA para séries temporais. |\n",
    "| statsmodels.tsa.arima.model.ARIMA | Classe para implementação manual de modelos ARIMA. |\n",
    "| statsmodels.tsa.stattools  | Funções para testes de estacionariedade de séries temporais (ADF, KPSS). |\n",
    "| statsmodels.tsa.seasonal | Função para decomposição sazonal de séries temporais. |\n",
    "| statsmodels.graphics.tsaplots | Funções para plotagem de funções de autocorrelação (ACF) e autocorrelação parcial (PACF) de séries temporais. |\n",
    "| sklearn.metrics | Funções para cálculo de métricas de erro em tarefas de previsão (MAE, RMSE, MAPE). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições Padrões para uso do matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuração acima foi definida para manter a escala de 12 por 5 para todos os plot do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando os dados do IBOVESPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ibovespa 2004 a 2024.csv\", sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Os dados acima foram importados do site investing através do link (https://br.investing.com/indices/bovespa-historical-data) conforme solicitado no tech challenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualização das 5 primeiras linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização das 5 ultimas linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização das informações da tabela, foi notado que o campo data está como objet em vez de datetime[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização de dados estastistico basico da base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir do dataframe DF foi criado uma Serie Fechamento, o campo data foi formatado para datetime e definido como index da serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_fechamento = pd.Series(data=df['Último'].values, index=pd.to_datetime(df['Data'], format='%d.%m.%Y'))\n",
    "serie_fechamento = serie_fechamento.sort_index(ascending=True)\n",
    "serie_fechamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criamos uma função reutilizável para exibir séries. Ela permite personalizar o título e as legendas, que podem ser passadas como listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotarSerie(serie, titulo=\"Valores da bolsa de valores com base no fechamento diário\", legendas=[]):\n",
    "    serie.plot()    \n",
    "    plt.title(titulo)\n",
    "    plt.xlabel('Data')\n",
    "    plt.grid(visible=True, linestyle='-', linewidth=0.7)\n",
    "    plt.ylabel('Valor em Real (R$)')\n",
    "    if legendas:\n",
    "        plt.legend(labels=legendas, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feito um primeiro plot utilizando o metodo *plotarSerie(serie, titulo, legendas)* para a serie fechamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_fechamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Já é possível notar que a serie tem tendencia de crescimento e a principio não aparenta ser uma serie estácionaria, devido sua crescente ao longo do tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A seguir, foi desenvolvido um novo método que irá gerar gráficos da série temporal, incluindo a média móvel mensal, trimestral e anual.\n",
    "> Obs: estamos considerando que os dias uteis do mes são 22 dias. portanto os numeros do metódo são baseado em dias úteis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_medias_moveis(serie, titulo ='Valores da bolsa de valores com média movel' ):\n",
    "    dias_uteis = 22\n",
    "    serie_media_movel_30 = serie.rolling(dias_uteis).mean()\n",
    "    serie_media_movel_90 = serie.rolling(dias_uteis * 3).mean()\n",
    "    serie_media_movel_anual = serie.rolling(dias_uteis * 12).mean()\n",
    "    plotarSerie(serie)\n",
    "    plotarSerie(serie_media_movel_30)\n",
    "    plotarSerie(serie_media_movel_90)\n",
    "    plotarSerie(serie_media_movel_anual, titulo ,['Valor Real','Média movel mensal', 'Média Móvel trimestral', 'Média Móvel Anual'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando o método descrito anteriormente, vamos plotar a série de fechamento para analisar visualmente sua tendência ao longo do tempo. Para isso, adicionaremos uma linha representando a média móvel de X períodos, o que nos permitirá identificar padrões e possíveis anomalias nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_medias_moveis(serie_fechamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora que já visualizamos os dados, vamos entender como está a distribuição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(serie_fechamento, dist='norm', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Avaliando a distribuição de dados Q-Q plot (quantile-quantile plot), é possível notar que a distribução dos dados não está normalizada, o esperado é que os pontos azuis, estivessem muitos proximos da linha vermelha. Neste gráfico, os dados começam a se alinhar com a linha vermelha nos quantis intermediários, mas desviam-se nas caudas (extremos), indicando que a distribuição dos dados pode não ser perfeitamente normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(serie_fechamento, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Uma segunda forma de avaliação é utilizando o histograma, nele é possível visualizar que existem picosem torno dos 20 60 e 110, aparenta ter grupos distintos de dados ou várias distribuições sobre postas, confirmando que os dados não seguem uma distribuição normal. um grafico de distribuição normal, tem uma aparencia em formato de um sino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de normalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Muitas vezes as observações de um grafico aparenta ter uma distribuição normais, mas isso não garante que os dados tenho uma distribuição normal, para garantir a distribuição podemos fazer o teste de **Shapiro Wilk**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Shapiro-Wilk\n",
    "\n",
    "### O que é o teste de Shapiro-Wilk?\n",
    "\n",
    "    O teste de Shapiro-Wilk é uma ferramenta estatística utilizada para avaliar se um conjunto de dados se ajusta a uma distribuição normal. A distribuição normal, também conhecida como curva em forma de sino, é uma das distribuições mais comuns em estatística e é frequentemente utilizada em diversos modelos estatísticos.\n",
    "\n",
    "#### Por que é importante testar a normalidade?\n",
    "\n",
    "    Muitos testes estatísticos assumem que os dados seguem uma distribuição normal. Se essa premissa não for verdadeira, os resultados do teste podem ser enganosos. Ao realizar o teste de Shapiro-Wilk, você verifica se essa suposição é razoável para seus dados.\n",
    "\n",
    "Critérios:\n",
    "\n",
    "Nível de significância de 0,05 ou 5% (mais utilizado)\n",
    "\n",
    "Valor de p < 0.05: Os dados provavelmente não são normalmente distribuídos.\n",
    "Valor de p ≥ 0.05: Não há evidência suficiente para rejeitar a hipótese de normalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abaixo criamos um método que calcula a partir de uma serié se a distribuição é normal ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_distribuicao(serie):\n",
    "    print(colored(':: Validação Distribuição (Shapiro-Wilk) ::', 'light_blue', attrs=[\"bold\"]))\n",
    "    e, p = stats.shapiro(serie)\n",
    "    print(f'Estátistica do teste: {e}')\n",
    "    print(f'p-valor: {p}')\n",
    "    if p > e:\n",
    "        print(colored('>> Distribuição Normal <<','green'))\n",
    "    else:\n",
    "        print(colored('Não há evidência suficiente para rejeitar a hipótese de normalidade','red'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_distribuicao(serie_fechamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- Foi executado o teste de **Shapiro Wilk** e foi constatado que não há uma distribuição normal, mas podemos tentar buscar uma normalidade através de transformação logaritmica ou transformação de raiz quadrada ou cubica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de Transformação para Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para transformar os dados visando uma normalização (ou aproximação de uma distribuição normal), você pode tentar diferentes técnicas de transformação. Aqui estão algumas abordagens comuns:\n",
    "\n",
    "\n",
    "### 1. Transformação Logarítmica\n",
    "Utilizada para reduzir a assimetria positiva (cauda longa à direita).\n",
    "\n",
    "\\[\n",
    "    $y' = \\log(y)$\n",
    "\\]\n",
    "\n",
    "**Uso**: Quando os dados possuem muitos valores altos que puxam a média.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Transformação de Raiz Quadrada ou Cúbica\n",
    "Suaviza variações em dados positivos e assimétricos.\n",
    "\n",
    "\\[\n",
    "$y' = \\sqrt{y}$\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando a transformação com Logarítmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_log = np.log(serie_fechamento)\n",
    "serie_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(serie_log, dist='norm', plot=plt)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(serie_log, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Após a transformações a distribuição foi modificada, porem não teve bons resultados, pois ela ainda não teve sua distribuição normalizada, iremos aplica a validação de shapiro para confirmação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_distribuicao(serie_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Mesmo efetuando transformação em logaritmo não foi possível obter um bom resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Transformação de Raiz Quadrada ou Cúbica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos elevar ao cubo para tentar obter uma melhor distribuição, usaremos o sign e abs para evitar o valores negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_ao_cubo = np.sign(serie_fechamento)*abs(serie_fechamento)**(1/3)\n",
    "serie_ao_cubo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(serie_ao_cubo, dist='norm', plot=plt)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(serie_ao_cubo, kde=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_distribuicao(serie_ao_cubo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mesmo fazendo a transformação para Não foi possível obter a distribuição normal usando serie ao cubo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompondo a serie fechamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos começar a avaliar os dados separadamente, com objetivo de isolar os componentes para uma analise mais individual o que irá ajuda a identificar o que está impulsionando o comportamento da série, como padrões sazonais ou uma tendência geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_decomposta = seasonal_decompose(serie_fechamento, period=22)\n",
    "serie_decomposta.plot()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grafico a acima demonstra uma tendencia crescente ao longo do tempo, mas a sanonalidade fica dificil de avaliar, portanto vamos aumentar o periodo para melhorar a visualização da sazonalidade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_decomposta = seasonal_decompose(serie_fechamento, model=\"multiplicative\", period=252)\n",
    "serie_decomposta.plot()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Após aumentar o periodo foi possível identificar melhor que existe uma sazonalidade entre os dados e os residuos tambem ficou mais claro\n",
    "- Os resíduos representam o ruído da série temporal, ou seja, as variações que não são previsíveis com base nos componentes sistemáticos (tendência e sazonalidade, por exemplo). Essas flutuações podem ser devidas a eventos externos, fatores aleatórios ou variações de curto prazo.\n",
    "- Tantos nos dados observados, tendencia e nos residos é possível notar duas quedas, entre os anos de 2008-2009 e 2020.\n",
    "- Essas quedas foram acometidas por eventos que impactou a economia mundial, sendo o primeiro deles em 2008 que ocorreu a **crise financeira global** esse evento teve um impacto profundo na economia global e nos mercados financeiros, levando a uma das maiores quedas nas bolsas de valores em todo o mundo desde a Grande Depressão de 1929.\n",
    "- E em 2020 foi a *pandemia* do da COVID-19. A disseminação rápida do novo coronavírus **(SARS-CoV-2)** causou uma crise global de saúde pública, levando a grandes incertezas econômicas e a um colapso nos mercados financeiros. A crise desencadeada pela COVID-19 afetou praticamente todas as economias e setores ao redor do mundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para uma observação mais proximas dos dias atuais, iremos observar a sazonalidade nos seus ultimos 500 dias equivalente a 2 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_decomposta.seasonal.iloc[-500:].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "em 2 anos é possível notar que existe uma sazonalidade entre esse periodo observado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estacionaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **estacionaridade** é um conceito fundamental em séries temporais e análises estatísticas, pois tem implicações significativas para a modelagem e previsão de dados.\n",
    "\n",
    "- Uma série temporal é dita estacionária se suas propriedades estatísticas, como média, variância e autocovariância, permanecem constantes ao longo do tempo. Isso significa que as flutuações e padrões observados em uma parte da série serão semelhantes em outras partes.\n",
    "- Muitos modelos estatistico  pressupoe que os dados são estacionários, tendo assim uma previsão mais precisa sobre os dados\n",
    "- A **estacionaridade** permite detectar padrões tendencias e sazonalidades que são essenciais para analise e previsão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através da decomposição é possível notar que existe uma sazonalidade e uma tendencia de crescimento, que aparenta não ser estácionada.\n",
    "Para validar iremos um o test de KPSS e Dict Fuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste KPSS (Kwiatkowski-Phillips-Schmidt-Shin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O teste KPSS (Kwiatkowski-Phillips-Schmidt-Shin) é um método estatístico para verificar a estacionaridade de séries temporais. Sua hipótese nula assume que a série é estacionária, enquanto a alternativa sugere que é não estacionária. O teste calcula um estatístico KPSS com base nos resíduos da série e compara com valores críticos para tomar decisões. Se o valor calculado for maior que o crítico, rejeita-se a estacionaridade. É amplamente utilizado em análises econômicas e financeiras para validar a adequação de modelos que presumem estacionaridade.\n",
    "\n",
    "- Assumos então os seguintes critérios: \n",
    "\n",
    "> Ho = não é estacionário: estatística do teste > valor crítico\n",
    "\n",
    "> Ha = é estacionário:  estatística do teste < valor crítico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação KPSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Para facilitar o reuso criamos um metodo que valida se a serie é ou não estacionária e exibe os valores estatisticos e criticos do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_kpss(serie):\n",
    "    print(colored(':: Validação KPSS (Kwiatkowski-Phillips-Schmidt-Shin) ::', 'light_blue', attrs=[\"bold\"]))\n",
    "    resultado = kpss(serie)\n",
    "    estatistica_teste = resultado[0]\n",
    "    p_valor = resultado[1]\n",
    "    valores_criticos  = resultado[3]\n",
    "    percentil_referencia = '5%'\n",
    "\n",
    "    print(f'Estatística do teste: {estatistica_teste:.4f}')\n",
    "    print(f'p-valor {p_valor:.4f}')\n",
    "    print('Valores Críticos:')\n",
    "    table = PrettyTable(['Criticidade', 'Valor'])\n",
    "\n",
    "    for chave, valor in valores_criticos.items():\n",
    "        table.add_row([chave, valor])\n",
    "    \n",
    "    print(table)\n",
    "\n",
    "    if estatistica_teste > valores_criticos[percentil_referencia]:\n",
    "        print(colored('>> Serie não estacionária <<', 'red'))\n",
    "    else:\n",
    "        print(colored('>> Série estácionaria <<', 'green'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando a validação KPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_kpss(serie_fechamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Na aplicação da serie fechamento foi identificada que ela não é estacionária, pois o valor este estátistico é maior que o valor critico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste ADF (Dickey-Fuller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O teste de Dickey-Fuller é uma ferramenta estatística usada para verificar se uma série temporal possui raiz unitária, ou seja, se ela é estacionária. \n",
    "* A estacionariedade é importante para a aplicação de muitos modelos econométricos. O teste compara uma estatística calculada com um valor crítico. Se a estatística for menor que o valor crítico, rejeitamos a hipótese de raiz unitária e concluímos que a série é estacionária. \n",
    "* O teste de Dickey-Fuller aumentado (ADF) é a versão mais comum e inclui termos de defasagem para capturar a autocorrelação. A não estacionariedade pode levar a resultados enganosos em modelos econométricos e comprometer a qualidade das previsões.\n",
    "\n",
    "> H0: Se a estatística de teste for menor que o valor crítico, rejeitamos a hipótese nula e concluímos que a série é estacionária.\n",
    "> H0: Se a estatística de teste for maior ou igual ao valor crítico, não rejeitamos a hipótese nula e concluímos que a série pode ter raiz unitária (ou seja, não há evidências suficientes para afirmar que ela é estacionária).\n",
    "\n",
    "Usaremos como valor de referencia 5%\n",
    "\n",
    "Resumo: \n",
    "- p-valor > 0.05  \n",
    "    - Se p-valor for maior que o valor de referencia: Não rejeitar a Hipótese Nula: a série não é estacionária \n",
    "- p-valor < 0.05\n",
    "    - se p-valor for menor que o valor de referencia: Rejeitar a Hipótese Nula: a série é estacionária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_adfuller(serie):\n",
    "  print(colored(':: Validação ADF (Dickey-Fuller) ::', 'light_blue', attrs=[\"bold\"]))\n",
    "  result_adf = adfuller(serie)\n",
    "  statistic_value = result_adf[0]\n",
    "  p_value = result_adf[1]\n",
    "  critical_values =  result_adf[4]\n",
    "  valor_referencia = 0.05\n",
    "\n",
    "  print(f\"ADF Statistic: {statistic_value}\")\n",
    "  print(f'Valor-p do Teste ADF: {p_value}')\n",
    "\n",
    "  table = PrettyTable(['Criticidade','Valor'])\n",
    "  for key, value in critical_values.items():\n",
    "    table.add_row([key, value])\n",
    "\n",
    "  print(table)\n",
    "\n",
    "  if p_value > valor_referencia:\n",
    "    print(colored('Não rejeitar a Hipótese Nula: a série não é estacionária\\n','red'))\n",
    "  else:\n",
    "    print(colored('Rejeitar a Hipótese Nula: a série é estacionária\\n','green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_adfuller(serie_fechamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos o teste de ADF para validar se a serie é ou não estacionária e foi respondido que a serie nao é estácionaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliando a autocorrelação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(serie_fechamento)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As barras verticais mostram os valores de autocorrelação muito próximos de 1 para todos os lags, exibindo um sinal claro que a série não é estacionária, isso demonstram um padrão de decaimento lento ou até mesmo ausencia de decaimento, como a autocorrelação mantem um valor elevado demonstra que existe uma forte dependencia temporal entre os pontos da série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(serie_fechamento)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A autocorrelação parcial apresenta que o valor atual da série está formente relacionado ao valor anterior, com essa informação é possível notar que os valores mais distantes no passado tem pouca influencia na previsão do valor atual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após todas as analises concluimos que a serie não é estácionaria e que nessecitamos transforma-las em estacionarias para conseguir ter um melhor aproveitamento dos modelos que iremos criar, então iremos aplicar a  técnica de diferenciação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferenciação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A técnica de diferenciação é uma técnica utilizada para transformar uma série temporal não estacionária em uma série estacionária.\n",
    "- Ela consiste em subtrair o valor de uma observação do valor da observação anterior. Essa operação remove tendências e sazonalidades presentes na série original, tornando-a mais adequada para a aplicação de modelos econométricos e de previsão. \n",
    "- Ao diferenciar uma série, estamos analisando a taxa de variação ao invés dos níveis absolutos dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando Diferenciação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_fechamento_diff = serie_fechamento.diff()\n",
    "serie_fechamento_diff = serie_fechamento_diff.dropna()\n",
    "serie_fechamento_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foi aplicado a diferenciação e excluido a primeira linha que estava nula, justamente porque na diferenciação se perde o primeiro valor, pois ele não teve nenhum valor para comparar com o anterior tornando-se nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_fechamento_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Após a transformação percebemos que houve uma mudança no comportamento da série, é possível notar a ausencia de tendencia, isso significa que a média e a variancia da série são constantes ao longo do tempo, possívelmente ela se transformou em uma série estácionada, mas para isso iremos aplicar novamente os testes KPSS e o ADF para validar se realmente ela se tornou estácionaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_kpss(serie_fechamento_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O Resultado da transformação foi positivo, o teste **KPSS** informa que conseguimos tranformar ela em estácionaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao_adfuller(serie_fechamento_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O Resultado da transformação foi positivo, o teste **ADF** informa que conseguimos tranformar ela em estácionaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conseguimos através da diferenciação transformar a serie em estácionaria, agora podemos avaliar o comportamento da serie novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(serie_fechamento_diff, kde=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Avaliando o histograma acima é possivel notar a diferença da serie, mostrando que os dados estão mais centralizado proximo a zero, com formato de sino, indicando uma normalidade entre os dados não perfeita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposição da serie Diferencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_diff_decomposta = seasonal_decompose(serie_fechamento_diff, period=365)\n",
    "serie_diff_decomposta.plot()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Após a tranformação tambem é possível notar que não há mais a tendencia de alta e baixa e que os residuos estão estão aleatorios e sem padrões claros, porem é possível notar que os anos de 2008-2009 e 2020 continuam influenciar os graficos de observação, tendencia e residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Avalição residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(serie_diff_decomposta.resid, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conseguimos avaliar visualmente que o residuo aparenta ter uma distribuição bem proxima do normal com uma concentração de valores bem proximas a zero, porem a cauda indica possíveis outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_medias_moveis(serie_fechamento_diff, 'Valores da bolsa de valores com média movel com diferenciação')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Na imagem acima conseguimos visualizar o valor real diferenciado e seus médias moveis mensal, trimestral e anual, onde ela demonstra se tornado estacionária, não tendo evidencias de tendencias, ausencia de padrões sazonais nas médias móveis que sugere tambem que a sazonalidade tambem foi removida e que a variabilidade da série parece ser constante ao longo do tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_medias_moveis(serie_fechamento_diff.loc[pd.Timestamp('2020-01-02'):pd.Timestamp('2023-12-29')], 'Valores da bolsa de valores com média movel com diferenciação de 2020 a 2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Na imagem acima tem a mesmas caracteristica da imagem anterior, a diferença é que estamos vendo os ultimos 4 anos o que torna mais facil a visualização e entendimento do gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelação com diferenciação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(serie_fechamento_diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A autocorrelação da diferenciação apresentou um resultado satisfatórios, pois foi possivel notar que a partir do indicador 2 a correlação se manteve proximo ao intervalo de confiança, exceto por um item fora, porem aceitavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(serie_fechamento_diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlação parcial tambem teve seus bon resultados considerendo-se que tambem a partir do 2 ponto se manteve dentro ou bem proximo do intervalo de confiança. Com esse resultado já é possível utilizar um modelo ARIMA com ordem 2 baseado nesse resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do Modelo ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoRegressive Integrated Moving Average (ARIMA)\n",
    "*É um modelo estatístico amplamente utilizado para analisar e prever séries temporais. As letras que compõem essa sigla possuem significados específicos:*\n",
    "* AR (AutoRegressiva): Indica que o valor da série em um determinado momento é uma função linear de seus valores anteriores. Ou seja, os valores passados influenciam os valores futuros.\n",
    "* I (Integrada): Refere-se ao processo de diferenciação aplicado à série temporal. A diferenciação é uma técnica utilizada para tornar a série estacionária, removendo tendências ou sazonalidades.\n",
    "* MA (Média Móvel): Indica que o valor da série em um determinado momento é uma função linear dos erros aleatórios (ruídos) ocorridos em momentos anteriores\n",
    "\n",
    "\n",
    "\n",
    "*Um modelo ARIMA é representado por três números: ARIMA(p,d,q).*\n",
    "* p: Ordem do processo autoregressivo. Indica o número de períodos anteriores que são usados para prever o valor atual.\n",
    "* d: Grau de diferenciação. Indica o número de vezes que a série é diferenciada para torná-la estacionária.\n",
    "* q: Ordem do processo de médias móveis. Indica o número de termos de erro anteriores que são incluídos no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considerando os valores encontrados nos resultados anteriores, iremos começar testando o modelo com os seguintes valores p = 2, d = 1, e q = 1\n",
    "\n",
    "- sendo *p = 2*  que o valor atual vai depender dos 2 valores anteriores, *d = 1* porque usamos a diferenciação uma unica vez e  *q = 1*  que o erro atual depende do erro anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,d,q =  2,1,1\n",
    "modelo_arima = SARIMAX(serie_fechamento_diff, order = (p,d,q), trend='c', enforce_stationarity=True)\n",
    "resultado = modelo_arima.fit()\n",
    "print(resultado.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O IAC encontrado foi de 15174, porem tem alguns warning no qual podemos avaliar \n",
    "1. Ele indica que a index de data, não tem uma frequencia, isso ocorre porque na nossa serié temos alguns dias faltando, como feriados e finais de semana, o que impede de ter todos os dias do ano e não conseguir colocar uma frequencia diaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevendo 10 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inicial = '2024-03-12'\n",
    "index = pd.date_range(start=data_inicial, periods=10)\n",
    "forecast_arima = resultado.forecast(10)\n",
    "forecast_arima\n",
    "resultado_previsao_arima = pd.Series(data=forecast_arima.values, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- usamos o metodo forecast para prever os proximos 10 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_previsao_arima.plot()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A previsão conseguiu prever os 2 primeiros valores, e depois disso entro em uma linha reta, isso indica que o nosso modelo não está conseguindo prever muito bem com os nossos dados. vamos aplicar o resultado como Garch para tentar melhorar o resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando o GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (Heterocedasticidade Condicional AutoRegressiva Generalizada) é um modelo estatístico utilizado para modelar a volatilidade de séries temporais financeiras. Ele assume que a variância dos retornos de um ativo não é constante ao longo do tempo, mas sim dependente de suas próprias realizações passadas. Em outras palavras, o GARCH captura a ideia de que períodos de alta volatilidade tendem a ser seguidos por outros períodos de alta volatilidade, e o mesmo ocorre com períodos de baixa volatilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iremos usar o residuo do resultado do modelo, o mean = Zero que estamos assumindo qu e a média dos residuos é zero, o vol como 'GARCH', porque queremos usar um modelo GARCH para modelar a volatividade dos residuos que é capaz de capturar a variancia dos erros ao longo do tempo, e os mesmos p e q usado no ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = resultado.resid\n",
    "model_garch = arch.arch_model(resid, mean='Zero', vol='GARCH', p=p, q=q).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após sua finalização iremos tentar prever novamente, só que agora com 30 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inicio, data_fim = '2024-03-12','2024-04-10'\n",
    "forecast_arima = resultado.forecast(steps=30)\n",
    "forecast_garch = model_garch.forecast(horizon=30)\n",
    "forecast = forecast_arima + np.sqrt(forecast_garch.variance.values[-1, :]) * np.random.normal(size=30)\n",
    "serie_prevista_garch = pd.Series(data=forecast.values, index=pd.date_range(data_inicio, data_fim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fizemos novamente o forecast tando do arima tanto do garch e juntamos os valores num só forcast e criamos uma serie prevista com garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(serie_fechamento_diff, label='REAL')\n",
    "plt.plot(serie_prevista_garch, label='Previsão ARIMA + GARCH')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fizemos o plot dos valores reais + previsto e o resultado foi satisfatorio, veremos mais a seguir um pouco mais de perto como ficou a previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(serie_fechamento_diff.loc[pd.Timestamp('2023-01-02'):pd.Timestamp('2024-03-12')], label='REAL')\n",
    "plt.plot(serie_prevista_garch, label='Previsão ARIMA + GARCH')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na imagem acima deu para notar a que a previsão conseguiu manter a previsão adequada e seguindo conforme o esperado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversão diferenciação da previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverter_diferenciacao(serie,serie_diferenciada):\n",
    "\n",
    "    ultimo_valor = serie.iloc[-1]\n",
    "    serie_normalizada = ultimo_valor + serie_diferenciada.cumsum()\n",
    "    serie_normalizada\n",
    "\n",
    "    return serie_normalizada    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsao_normal = reverter_diferenciacao(serie_fechamento, serie_prevista_garch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_fechamento.loc[pd.Timestamp('2023-01-02'):pd.Timestamp('2024-03-12')])\n",
    "plotarSerie(previsao_normal, titulo='Dados reais + previsão arima + Garch', legendas=['Série Real', 'Previsão arima + Garch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação da desempenho do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_avaliacao = 30\n",
    "treino_arima = serie_fechamento_diff.head(len(serie_fechamento_diff) - dias_avaliacao)\n",
    "treino_arima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arima = serie_fechamento_diff.tail(dias_avaliacao)\n",
    "test_arima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,d,q =  2,1,1\n",
    "\n",
    "modelo_arima = SARIMAX(treino_arima, order = (p,d,q), trend='c', enforce_stationarity=True)\n",
    "\n",
    "resultado = modelo_arima.fit(maxiter=100)\n",
    "\n",
    "print(resultado.summary())\n",
    "\n",
    "forecast_arima = resultado.forecast(dias_avaliacao)\n",
    "\n",
    "resid = resultado.resid\n",
    "model_garch = arch.arch_model(resid, mean='Zero', vol='GARCH', p=p, q=q).fit()\n",
    "forecast_garch = model_garch.forecast(horizon=dias_avaliacao)\n",
    "\n",
    "forecast_garch\n",
    "forecast = forecast_arima + np.sqrt(forecast_garch.variance.values[-1, :]) * np.random.normal(size=dias_avaliacao)\n",
    "serie_prevista_garch = pd.Series(data=forecast.values, index=test_arima.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de metricas (Erro Médio Absoluto, Erro Quadrático Médio e Erro Percentual Médio Absoluto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **MAE (Erro Médio Absoluto):**  Mede a média dos erros absolutos entre os valores previstos e os reais. É uma métrica robusta que não é influenciada por outliers e fornece uma ideia geral da precisão do modelo.\n",
    "* **MSE (Erro Quadrático Médio):** Calcula a média dos erros ao quadrado. Penaliza mais os erros maiores, sendo útil quando grandes erros são mais problemáticos. É a base para o RMSE (raiz do erro quadrático médio), que é mais interpretado na mesma escala que os dados originais.\n",
    "* **MAPE (Erro Percentual Médio Absoluto):** Calcula a média dos erros percentuais absolutos. É útil para comparar a precisão de modelos em diferentes escalas, mas pode ser instável quando há valores próximos de zero.\n",
    "\n",
    "**Em resumo:** O MAE fornece uma medida de erro geral, o MSE penaliza mais erros grandes e o MAPE expressa o erro como uma porcentagem. A escolha da métrica depende do problema específico e da importância relativa de diferentes tipos de erros.\n",
    "\n",
    "Para facilitar o Reuso criamos 2 metodos. um que calcula as métricas e outro que imprime o valor das metricas com seus respectivos valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = root_mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    return mae, mse, mape\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    mae, mse, mape = metrics\n",
    "    print(colored(f\"MAE: {mae}\", 'light_blue'))\n",
    "    print(colored(f\"MSE: {mse}\", 'light_cyan'))\n",
    "    print(colored(f\"MAPE: {mape:.2f} %\", 'light_green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foi criado uma função para salvar em memória os resultados dos testes, para no final compararmos o modelo que obteve o melhor resultado nas previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = {}\n",
    "\n",
    "def salvar_metricas(modelo, new_metric):\n",
    "    mae, mse, mape = new_metric\n",
    "    m = {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "    \n",
    "    metricas[modelo]=m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter uma melhor metricas, iremos desfazer a diferenciação, colocando os dados na sua escala real, assim poderemos tirar melhor aproveitamento das metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normalizada = reverter_diferenciacao(serie_fechamento.head(len(serie_fechamento) - dias_avaliacao),test_arima )\n",
    "previsao_normalizada = reverter_diferenciacao(serie_fechamento.head(len(serie_fechamento) - dias_avaliacao),serie_prevista_garch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(test_normalizada.values, previsao_normalizada.values)\n",
    "print_metrics(metrics)\n",
    "salvar_metricas('Arima', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do modelo XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **XGBRegressor** é um algoritmo de *machine learning* poderoso e eficiente, especialmente utilizado para **problemas de regressão**. Ele funciona combinando múltiplas árvores de decisão (ensemble) de forma sequencial, aprendendo dos erros dos modelos anteriores. Essa abordagem, chamada de *gradient boosting*, permite que o XGBRegressor construa modelos altamente precisos e robustos. \n",
    "\n",
    "Embora o XGBRegressor não tenha sido especificamente projetado para séries temporais, ele pode ser adaptado para esse tipo de problema com excelentes resultados. Isso se deve a sua capacidade de:\n",
    "\n",
    "**Capturar padrões complexos:** As árvores de decisão do XGBoost são capazes de capturar padrões não-lineares e interações entre diferentes características, o que é comum em séries temporais.\n",
    "\n",
    "**Lidar com grandes volumes de dados:** O algoritmo é otimizado para lidar com grandes conjuntos de dados, o que é frequentemente o caso em séries temporais.\n",
    "\n",
    "**Alta precisão:** Em diversas competições de machine learning, o XGBoost tem demonstrado ser um dos algoritmos mais precisos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No método abaixo estamos criando uma função para preparar os dados para o nosso modelo, separamos a data por ano, mes,dia e dia da semana, e renomeamos duas colunas, Último para valor_fechamento e Abertura para valor_abertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1936,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(dado):\n",
    "    data_frame = pd.DataFrame(dado)\n",
    "    data_frame['valor_fechamento']  = data_frame['Último']\n",
    "    data_frame['valor_abertura']  = data_frame['Abertura']\n",
    "    data_frame['Data'] = pd.to_datetime(data_frame['Data'], format='%d.%m.%Y')\n",
    "    data_frame['year'] = data_frame[\"Data\"].dt.year\n",
    "    data_frame['month'] = data_frame[\"Data\"].dt.month\n",
    "    data_frame['day'] = data_frame[\"Data\"].dt.day\n",
    "    data_frame['dayofweek'] = data_frame[\"Data\"].dt.dayofweek\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função criada, criamos duas novas variaveis, que é o feature_names no qual será as variaveis que o modelo irá usar do dataframe e o target, que é o valor que o modelo tentará prever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1937,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"year\", \"month\", \"day\", \"dayofweek\",\"valor_abertura\"]\n",
    "target = \"valor_fechamento\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Em seguida, criaremos um DataFrame de features e o ordenaremos por data para preparar os dados para a divisão em conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = create_feature(df)\n",
    "df_feature = df_feature.sort_values(by='Data')\n",
    "df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com o DataFrame de features pronto, podemos dividir os dados em conjuntos de treino e teste. Utilizaremos a função **train_test_split** do scikit-learn para essa tarefa. A variável **X** conterá as features (características) que o modelo utilizará para fazer as previsões, enquanto **y** conterá o target (valor que queremos prever). Definiremos **test_size=0.0059** para que o conjunto de teste represente os últimos 30 dias dos nossos dados (considerando um DataFrame com 5000 registros). Ao configurar **shuffle=False**, garantimos que a ordem temporal dos dados seja preservada, evitando que os dados de teste 'contaminem' o conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feature[feature_names]\n",
    "y = df_feature['valor_fechamento']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0059, shuffle=False)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos treino e teste e temos 4970 registros para teste e 30 para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando as 5 primeiros linhas do treino deu para validar que os dados começaram de 2004 conforme o  esperavamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando suas 5 últimas linhas do treino foi possível notar que os ultimos dias foram janeiro de 2024 conforme o esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o nosso X_test tambem ficou adequado, começando final de janeiro até dia 12 de março"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um modelo XGBRegressor configurado para minimizar o erro quadrático médio (\"reg:squarederror\").\n",
    "- Treinamos o modelo com os dados de treino (`X_train`, `y_train`). O modelo aprende a relação entre as features (`X_train`) e o target (`y_train`).\n",
    "- Utilizamos o modelo treinado para fazer previsões nos dados de teste (`X_test`). A variável `predict_values` armazena as previsões do modelo para cada ponto no conjunto de teste.\n",
    "- `predict_values` contém as previsões do modelo para o target nos dados de teste, que podemos comparar com o target real para avaliar a performance do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1943,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "predict_values = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados já previstos, iremos calcular as metricas para entender como foi a performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(y_true=predict_values, y_pred=y_test.values)\n",
    "print_metrics(metrics)\n",
    "salvar_metricas('XGBRegressor', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O erro médio absoluto (MAE) indica que, em média, as previsões estão desviando cerca de 1.68 unidades do valor real. \n",
    "\n",
    "O erro quadrático médio (MSE) sugere que os erros maiores estão sendo penalizados.\n",
    "\n",
    "O erro percentual absoluto médio (MAPE) indica um erro médio de aproximadamente 1.32%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos reconstruir a data da serie treino para que seja possível visualizar os dados com os valores previstos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"data\"] = X_train[\"year\"].map(str) + \"-\" + X_train[\"month\"].map(str) + \"-\" + X_train[\"day\"].map(str)\n",
    "serie_treino = pd.Series(data=y_train.values, index=pd.to_datetime(X_train['data']))\n",
    "serie_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo as datas dos valores previsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"data\"] = X_test[\"year\"].map(str) + \"-\" + X_test[\"month\"].map(str) + \"-\" + X_test[\"day\"].map(str)\n",
    "serie_prevista_xgb = pd.Series(data=predict_values, index=pd.to_datetime(X_test['data']))\n",
    "serie_prevista_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_treino)\n",
    "plotarSerie(serie_prevista_xgb,titulo=\"Previsão bolsa de valores usando XGB Regressor\", legendas=['Valor Real', 'Valor Previsto'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_treino[-500:])\n",
    "plotarSerie(serie_prevista_xgb, titulo=\"Previsão bolsa de valores usando XGB Regressor\", legendas=['Valor Real', 'Valor Previsto'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do modelo Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **Prophet** é uma biblioteca desenvolvida pelo Facebook (agora Meta) voltada para a modelagem e previsão de **séries temporais**. Ele foi projetado para lidar com componentes sazonais, tendências e feriados de maneira intuitiva e eficiente. Uma de suas maiores vantagens é a simplicidade e a facilidade de uso, mesmo para aqueles que não possuem experiência avançada com modelagem estatística.\n",
    "\n",
    "Prophet se destaca por:\n",
    "\n",
    "- Ser robusto com dados ausentes.\n",
    "- Gerenciar mudanças de tendência.\n",
    "- Considerar sazonalidades e eventos especiais, como feriados.\n",
    "- Funcionar bem com séries temporais diárias, semanais e anuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação dos dados para utilização com o modelo, mantendo somente as datas (ds) e o target y (Último):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.DataFrame(df)\n",
    "df_p['ds'] = pd.to_datetime(df['Data'], format='%d.%m.%Y')\n",
    "df_p.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.rename(columns={'Último':'y'}, inplace=True)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p[['ds', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.sort_values(by='ds')\n",
    "df_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevendo os feriados do estado de São Paulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_feriados = holidays.Brazil(state='SP')\n",
    "def preve_feriado(data):\n",
    "    data_formatada = data.strftime('%Y-%m-%d')    \n",
    "    return data_formatada in sp_feriados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_p['dias_uteis'] = df_p['ds'].dt.dayofweek < 5 \n",
    "df_p['feriados'] = df_p['ds'].map(preve_feriado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a divisão dos dados em treino e teste, utilizando o SKLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "treino, teste = train_test_split(df_p, test_size=0.0059, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o modelo comos dados até o dia 26/01/2024, com sazonalidade diária:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_prop = Prophet(daily_seasonality=True)\n",
    "modelo_prop.add_regressor(\"dias_uteis\")\n",
    "modelo_prop.add_regressor(\"feriados\")\n",
    "modelo_prop.fit(treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustando os períodos a serem previstos conforme o tamanho do dataframe de testes, e configurando a coluna booleana de feriados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_prop = modelo_prop.make_future_dataframe(periods=len(teste))\n",
    "future_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_prop['dias_uteis'] = df_p['dias_uteis']\n",
    "future_prop['feriados'] = df_p['feriados']\n",
    "future_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a previsão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_prop = modelo_prop.predict(future_prop)\n",
    "forecast_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_prop['yhat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=forecast_prop['yhat'].values, index=forecast_prop['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito_prop = forecast_prop.tail(len(teste))[[\"ds\",'yhat']]\n",
    "predito_prop.rename(columns={'ds':'data_prevista'}, inplace=True)\n",
    "predito_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de um dataframe com os dados de teste e previstos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_df = pd.DataFrame(teste)\n",
    "novo_df['previsto'] = predito_prop['yhat'].values\n",
    "novo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo das métricas de desempenho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(y_true=teste['y'].values, y_pred=novo_df['previsto'])\n",
    "print_metrics(metrics)\n",
    "salvar_metricas('Prophet', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses valores sugerem que o modelo está apresentando um bom desempenho. Uma previsão com MAPE abaixo de 10% já pode ser considerada muito boa em muitos contextos, especialmente em séries temporais voláteis, como a de índices financeiros. O MAE relativamente baixo também sugere que o modelo está realizando previsões com uma margem de erro considerável para aplicações práticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_prop['yhat_upper'].values\n",
    "forecast_prop['yhat_lower'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de dataframe com valor real, previsto (yhat), e intervalos de confiança (yhat_lower e yhat_upper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_final  = { \n",
    "                    'valor_real': df_p['y'].values, \n",
    "                    'valor_previsto': forecast_prop['yhat'].values, \n",
    "                    'valor_previsao_maxima': forecast_prop['yhat_upper'].values, \n",
    "                    'valor_previsao_minima': forecast_prop['yhat_lower'].values\n",
    "                    }\n",
    "\n",
    "df_resultado  = pd.DataFrame(data=resultado_final, index=df_p['ds'])\n",
    "df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gráficos:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(df_resultado['valor_real'])\n",
    "plotarSerie(df_resultado['valor_previsto'])\n",
    "plotarSerie(df_resultado['valor_previsao_maxima'])\n",
    "plotarSerie(df_resultado['valor_previsao_minima'],titulo=\"Previsão usando Prophet\", legendas=['Valor Real', 'Valor Previsto', 'Intervalo de confiança acima', 'Intervalo de confiança abaixo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(df_resultado['valor_real'].tail(365))\n",
    "plotarSerie(df_resultado['valor_previsto'].tail(365))\n",
    "plotarSerie(df_resultado['valor_previsao_maxima'].tail(365))\n",
    "plotarSerie(df_resultado['valor_previsao_minima'].tail(365),titulo=\"Previsão dos últimos 365 períodos usando Prophet\", legendas=['Valor Real', 'Valor Previsto', 'Intervalo de confiança acima', 'Intervalo de confiança abaixo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_prop.plot(forecast_prop)\n",
    "plt.legend(loc='best', labels=['Pontos observados','Previsão', 'Intervalo de confiança'])\n",
    "plt.ylabel('Valor ação')\n",
    "plt.xlabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_modelos = pd.DataFrame(metricas).T\n",
    "resultado_modelos.head().sort_values(by=\"mape\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até o momento, o modelo com melhor desempenho foi o **XGBRegressor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do modelo XGBRegressor com maior tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **XGBRegressor** é um algoritmo de *machine learning* poderoso e eficiente, especialmente utilizado para **problemas de regressão**. Ele funciona combinando múltiplas árvores de decisão (ensemble) de forma sequencial, aprendendo dos erros dos modelos anteriores. Essa abordagem, chamada de *gradient boosting*, permite que o XGBRegressor construa modelos altamente precisos e robustos. \n",
    "\n",
    "Embora o XGBRegressor não tenha sido especificamente projetado para séries temporais, ele pode ser adaptado para esse tipo de problema com excelentes resultados. Isso se deve a sua capacidade de:\n",
    "\n",
    "**Capturar padrões complexos:** As árvores de decisão do XGBoost são capazes de capturar padrões não-lineares e interações entre diferentes características, o que é comum em séries temporais.\n",
    "\n",
    "**Lidar com grandes volumes de dados:** O algoritmo é otimizado para lidar com grandes conjuntos de dados, o que é frequentemente o caso em séries temporais.\n",
    "\n",
    "**Alta precisão:** Em diversas competições de machine learning, o XGBoost tem demonstrado ser um dos algoritmos mais precisos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No método abaixo estamos criando uma função para preparar os dados para o nosso modelo, separamos a data por ano, mes,dia e dia da semana, e renomeamos duas colunas, Último para valor_fechamento e Abertura para valor_abertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2019,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(dado):\n",
    "    data_frame = pd.DataFrame(dado)\n",
    "    data_frame['valor_fechamento']  = data_frame['Último']\n",
    "    data_frame['valor_abertura']  = data_frame['Abertura']\n",
    "    data_frame['Data'] = pd.to_datetime(data_frame['Data'], format='%d.%m.%Y')\n",
    "    data_frame['year'] = data_frame[\"Data\"].dt.year\n",
    "    data_frame['month'] = data_frame[\"Data\"].dt.month\n",
    "    data_frame['day'] = data_frame[\"Data\"].dt.day\n",
    "    data_frame['dayofweek'] = data_frame[\"Data\"].dt.dayofweek\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função criada, criamos duas novas variaveis, que é o feature_names no qual será as variaveis que o modelo irá usar do dataframe e o target, que é o valor que o modelo tentará prever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2020,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"year\", \"month\", \"day\", \"dayofweek\",\"valor_abertura\"]\n",
    "target = \"valor_fechamento\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Em seguida, criaremos um DataFrame de features e o ordenaremos por data para preparar os dados para a divisão em conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = create_feature(df)\n",
    "df_feature = df_feature.sort_values(by='Data')\n",
    "df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com o DataFrame de features pronto, podemos dividir os dados em conjuntos de treino e teste. Utilizaremos a função **train_test_split** do scikit-learn para essa tarefa. A variável **X** conterá as features (características) que o modelo utilizará para fazer as previsões, enquanto **y** conterá o target (valor que queremos prever). Definiremos **test_size=0.0059** para que o conjunto de teste represente os últimos 30 dias dos nossos dados (considerando um DataFrame com 5000 registros). Ao configurar **shuffle=False**, garantimos que a ordem temporal dos dados seja preservada, evitando que os dados de teste 'contaminem' o conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feature[feature_names]\n",
    "y = df_feature['valor_fechamento']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0450, shuffle=False)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos treino e teste e temos 4970 registros para teste e 30 para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando as 5 primeiros linhas do treino deu para validar que os dados começaram de 2004 conforme o  esperavamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando suas 5 últimas linhas do treino foi possível notar que os ultimos dias foram janeiro de 2024 conforme o esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o nosso X_test tambem ficou adequado, começando final de janeiro até dia 12 de março"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um modelo XGBRegressor configurado para minimizar o erro quadrático médio (\"reg:squarederror\").\n",
    "- Treinamos o modelo com os dados de treino (`X_train`, `y_train`). O modelo aprende a relação entre as features (`X_train`) e o target (`y_train`).\n",
    "- Utilizamos o modelo treinado para fazer previsões nos dados de teste (`X_test`). A variável `predict_values` armazena as previsões do modelo para cada ponto no conjunto de teste.\n",
    "- `predict_values` contém as previsões do modelo para o target nos dados de teste, que podemos comparar com o target real para avaliar a performance do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2026,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "predict_values = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados já previstos, iremos calcular as metricas para entender como foi a performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(y_true=predict_values, y_pred=y_test.values)\n",
    "print_metrics(metrics)\n",
    "salvar_metricas('XGBRegressor', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O erro médio absoluto (MAE) indica que, em média, as previsões estão desviando cerca de 1.68 unidades do valor real. \n",
    "\n",
    "O erro quadrático médio (MSE) sugere que os erros maiores estão sendo penalizados.\n",
    "\n",
    "O erro percentual absoluto médio (MAPE) indica um erro médio de aproximadamente 1.32%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos reconstruir a data da serie treino para que seja possível visualizar os dados com os valores previstos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"data\"] = X_train[\"year\"].map(str) + \"-\" + X_train[\"month\"].map(str) + \"-\" + X_train[\"day\"].map(str)\n",
    "serie_treino = pd.Series(data=y_train.values, index=pd.to_datetime(X_train['data']))\n",
    "serie_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo as datas dos valores previsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"data\"] = X_test[\"year\"].map(str) + \"-\" + X_test[\"month\"].map(str) + \"-\" + X_test[\"day\"].map(str)\n",
    "serie_prevista_xgb = pd.Series(data=predict_values, index=pd.to_datetime(X_test['data']))\n",
    "serie_prevista_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_treino)\n",
    "plotarSerie(serie_prevista_xgb,titulo=\"Previsão bolsa de valores usando XGB Regressor\", legendas=['Valor Real', 'Valor Previsto'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibovespa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarSerie(serie_treino[-500:])\n",
    "plotarSerie(serie_prevista_xgb, titulo=\"Previsão bolsa de valores usando XGB Regressor\", legendas=['Valor Real', 'Valor Previsto'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
